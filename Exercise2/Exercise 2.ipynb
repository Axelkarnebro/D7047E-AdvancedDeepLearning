{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\axelk/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\axelk/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "modelUT = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)\n",
    "modelUT = nn.Sequential(\n",
    "    modelUT,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 10)\n",
    ")\n",
    "print(modelUT.eval())\n",
    "modelUT.to(device)\n",
    "\n",
    "modelPT = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "for p in modelPT.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "modelPT = nn.Sequential(\n",
    "    modelPT,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 10)\n",
    ")\n",
    "print(modelPT.eval())\n",
    "modelPT.to(device)\n",
    "\n",
    "for p in modelPT.parameters():\n",
    "    print(p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    #transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(\n",
    "    train=True,\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    train=False,\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "train_set = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_set = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "def train_model(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(params = model.parameters(), lr = 0.01)\n",
    "    for epoch in range(epochs):\n",
    "        #print(f\"epoch: {epoch + 1}\")\n",
    "        run_loss = 0.0\n",
    "        model.train(True)\n",
    "\n",
    "        for batch_nr, (data, labels) in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = model(data)\n",
    "\n",
    "            loss = criterion(prediction, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "\n",
    "            run_loss += loss.item()\n",
    "\n",
    "        print(\"[%d, %5d], loss: %.3f \" % (epoch + 1, batch_nr + 1, run_loss / (batch_nr + 1)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   782], loss: 1.878 \n",
      "[2,   782], loss: 1.858 \n",
      "[3,   782], loss: 1.841 \n",
      "[4,   782], loss: 1.819 \n",
      "[5,   782], loss: 1.742 \n",
      "[6,   782], loss: 1.638 \n",
      "[7,   782], loss: 1.572 \n",
      "[8,   782], loss: 1.519 \n",
      "[9,   782], loss: 1.460 \n",
      "[10,   782], loss: 1.399 \n",
      "[11,   782], loss: 1.336 \n",
      "[12,   782], loss: 1.275 \n",
      "[13,   782], loss: 1.197 \n",
      "[14,   782], loss: 1.129 \n",
      "[15,   782], loss: 1.080 \n",
      "[16,   782], loss: 1.022 \n",
      "[17,   782], loss: 0.971 \n",
      "[18,   782], loss: 0.916 \n",
      "[19,   782], loss: 0.874 \n",
      "[20,   782], loss: 0.812 \n",
      "[21,   782], loss: 0.766 \n",
      "[22,   782], loss: 0.715 \n",
      "[23,   782], loss: 0.662 \n",
      "[24,   782], loss: 0.618 \n",
      "[25,   782], loss: 0.574 \n",
      "[26,   782], loss: 0.529 \n",
      "[27,   782], loss: 0.479 \n",
      "[28,   782], loss: 0.439 \n",
      "[29,   782], loss: 0.389 \n",
      "[30,   782], loss: 0.347 \n",
      "[31,   782], loss: 0.318 \n",
      "[32,   782], loss: 0.283 \n",
      "[33,   782], loss: 0.263 \n",
      "[34,   782], loss: 0.217 \n",
      "[35,   782], loss: 0.188 \n",
      "[36,   782], loss: 0.169 \n",
      "[37,   782], loss: 0.149 \n",
      "[38,   782], loss: 0.137 \n",
      "[39,   782], loss: 0.117 \n",
      "[40,   782], loss: 0.112 \n",
      "[41,   782], loss: 0.094 \n",
      "[42,   782], loss: 0.097 \n",
      "[43,   782], loss: 0.084 \n",
      "[44,   782], loss: 0.067 \n",
      "[45,   782], loss: 0.061 \n",
      "[46,   782], loss: 0.068 \n",
      "[47,   782], loss: 0.059 \n",
      "[48,   782], loss: 0.046 \n",
      "[49,   782], loss: 0.042 \n",
      "[50,   782], loss: 0.057 \n",
      "[51,   782], loss: 0.037 \n",
      "[52,   782], loss: 0.052 \n",
      "[53,   782], loss: 0.056 \n",
      "[54,   782], loss: 0.018 \n",
      "[55,   782], loss: 0.021 \n",
      "[56,   782], loss: 0.070 \n",
      "[57,   782], loss: 0.023 \n",
      "[58,   782], loss: 0.019 \n",
      "[59,   782], loss: 0.024 \n",
      "[60,   782], loss: 0.053 \n"
     ]
    }
   ],
   "source": [
    "trainedUT = train_model(modelUT, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   782], loss: 3.090 \n",
      "[2,   782], loss: 2.633 \n",
      "[3,   782], loss: 2.584 \n",
      "[4,   782], loss: 2.427 \n",
      "[5,   782], loss: 2.497 \n",
      "[6,   782], loss: 2.431 \n",
      "[7,   782], loss: 2.442 \n",
      "[8,   782], loss: 2.392 \n",
      "[9,   782], loss: 2.420 \n",
      "[10,   782], loss: 2.411 \n",
      "[11,   782], loss: 2.366 \n",
      "[12,   782], loss: 2.414 \n",
      "[13,   782], loss: 2.414 \n",
      "[14,   782], loss: 2.360 \n",
      "[15,   782], loss: 2.356 \n",
      "[16,   782], loss: 2.311 \n",
      "[17,   782], loss: 2.326 \n",
      "[18,   782], loss: 2.342 \n",
      "[19,   782], loss: 2.356 \n",
      "[20,   782], loss: 2.325 \n",
      "[21,   782], loss: 2.339 \n",
      "[22,   782], loss: 2.317 \n",
      "[23,   782], loss: 2.361 \n",
      "[24,   782], loss: 2.300 \n",
      "[25,   782], loss: 2.326 \n",
      "[26,   782], loss: 2.302 \n",
      "[27,   782], loss: 2.325 \n",
      "[28,   782], loss: 2.313 \n",
      "[29,   782], loss: 2.287 \n",
      "[30,   782], loss: 2.332 \n",
      "[31,   782], loss: 2.282 \n",
      "[32,   782], loss: 2.296 \n",
      "[33,   782], loss: 2.279 \n",
      "[34,   782], loss: 2.341 \n",
      "[35,   782], loss: 2.324 \n",
      "[36,   782], loss: 2.273 \n",
      "[37,   782], loss: 2.335 \n",
      "[38,   782], loss: 2.322 \n",
      "[39,   782], loss: 2.360 \n",
      "[40,   782], loss: 2.286 \n",
      "[41,   782], loss: 2.330 \n",
      "[42,   782], loss: 2.275 \n",
      "[43,   782], loss: 2.293 \n",
      "[44,   782], loss: 2.291 \n",
      "[45,   782], loss: 2.319 \n",
      "[46,   782], loss: 2.317 \n",
      "[47,   782], loss: 2.229 \n",
      "[48,   782], loss: 2.342 \n",
      "[49,   782], loss: 2.297 \n",
      "[50,   782], loss: 2.267 \n",
      "[51,   782], loss: 2.311 \n",
      "[52,   782], loss: 2.313 \n",
      "[53,   782], loss: 2.319 \n",
      "[54,   782], loss: 2.328 \n",
      "[55,   782], loss: 2.307 \n",
      "[56,   782], loss: 2.284 \n",
      "[57,   782], loss: 2.322 \n",
      "[58,   782], loss: 2.254 \n",
      "[59,   782], loss: 2.273 \n",
      "[60,   782], loss: 2.326 \n"
     ]
    }
   ],
   "source": [
    "trainedPT = train_model(modelPT, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainedUT.state_dict(), \"./Exercise2/trainedUT\")\n",
    "torch.save(trainedPT.state_dict(), \"./Exercise2/trainedPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelUT.load_state_dict(torch.load(\"./Exercise2/trainedUT\"))\n",
    "modelPT.load_state_dict(torch.load(\"./Exercise2/trainedPT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader):\n",
    "    correct = 0\n",
    "    all = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_nr,(data,label) in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(data)\n",
    "\n",
    "            correct += (torch.argmax(pred,1)== label).sum()\n",
    "            \n",
    "            all += (label.size(0))\n",
    "    return correct / all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not pre-trained:  tensor(0.6927, device='cuda:0')  Pre-trained:  tensor(0.4099, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "accUT = accuracy(modelUT, test_set)\n",
    "accPT = accuracy(modelPT, test_set)\n",
    "\n",
    "print(\"Not pre-trained: \", accUT, \" Pre-trained: \", accPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\axelk/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): ReLU(inplace=True)\n",
       "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (33): ReLU(inplace=True)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1,out_channels=6,kernel_size=3, padding='same'),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=(2,2)),\n",
    "    nn.Conv2d(in_channels=6,out_channels=6,kernel_size=3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=(2,2)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6*6*6, 10),\n",
    "    nn.Softmax(1)\n",
    ")\n",
    "\n",
    "modelCNN.to(device)\n",
    "\n",
    "modelVGG = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=False)\n",
    "modelVGG = nn.Sequential(\n",
    "    modelVGG,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 10)\n",
    ")\n",
    "modelVGG.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "transformMNIST = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataMNIST = torchvision.datasets.MNIST(\n",
    "    train=True,\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=transformMNIST\n",
    ")\n",
    "test_dataMNIST = torchvision.datasets.MNIST(\n",
    "    train=False,\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=transformMNIST\n",
    ")\n",
    "train_setMNIST = DataLoader(train_dataMNIST, batch_size=64, shuffle=True)\n",
    "test_setMNIST = DataLoader(test_dataMNIST, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x227f4e74f40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_setMNIST)\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "img = images[0].numpy()\n",
    "print(images[0].shape)\n",
    "img = np.transpose(img, (1,2,0))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   938], loss: 0.064 \n",
      "[2,   938], loss: 0.049 \n",
      "[3,   938], loss: 0.039 \n",
      "[4,   938], loss: 0.030 \n",
      "[5,   938], loss: 0.025 \n",
      "[6,   938], loss: 0.020 \n",
      "[7,   938], loss: 0.016 \n",
      "[8,   938], loss: 0.013 \n",
      "[9,   938], loss: 0.011 \n",
      "[10,   938], loss: 0.010 \n"
     ]
    }
   ],
   "source": [
    "trainedMNIST = train_model(modelVGG, train_setMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainedMNIST.state_dict(), \"./Exercise2/trainedVGG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN.load_state_dict(torch.load(\"./Exercise2/trainedMNIST\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9918, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "accMNIST = accuracy(trainedMNIST, test_setMNIST)\n",
    "print(accMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data\\train_32x32.mat\n",
      "Using downloaded and verified file: data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "transformSVHN = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataSVHN = torchvision.datasets.SVHN(\n",
    "    split=\"train\",\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataSVHN = torchvision.datasets.SVHN(\n",
    "    split=\"test\",\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_setSVHN = DataLoader(train_dataSVHN, batch_size=64, shuffle=True)\n",
    "test_setSVHN = DataLoader(test_dataSVHN, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): ReLU(inplace=True)\n",
       "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (33): ReLU(inplace=True)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in modelVGG.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "modelVGG[2] = nn.Linear(1000, 10)\n",
    "modelVGG.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1145], loss: 2.073 \n",
      "[2,  1145], loss: 1.997 \n",
      "[3,  1145], loss: 1.974 \n",
      "[4,  1145], loss: 1.962 \n",
      "[5,  1145], loss: 1.952 \n",
      "[6,  1145], loss: 1.943 \n",
      "[7,  1145], loss: 1.936 \n",
      "[8,  1145], loss: 1.931 \n",
      "[9,  1145], loss: 1.926 \n",
      "[10,  1145], loss: 1.922 \n"
     ]
    }
   ],
   "source": [
    "trainedSVHN = train_model(modelVGG, train_setSVHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3881, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "accSVHN = accuracy(trainedSVHN, test_setSVHN)\n",
    "print(accSVHN)"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
