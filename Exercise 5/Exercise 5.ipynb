{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "#import tensorflow_datasets as tfds\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "execution_count":5,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3\n",
    "X_dim = 28*28\n",
    "y_dim = 1\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=mb_size, shuffle=True)"
   ],
   "execution_count":36,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "data = tf.keras.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = data\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "test = next(train_dataset.batch(mb_size).as_numpy_iterator())\n",
    "X_dim = 28*28\n",
    "y_dim = 1\n",
    "print(X_dim,y_dim)\n",
    "h_dim = 128\n",
    "c = 0\n",
    "lr = 1e-3"
   ],
   "execution_count":37,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "784 1\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. \/ np.sqrt(in_dim \/ 2.)\n",
    "    xavier = torch.randn(*size) * xavier_stddev\n",
    "    xavier.requires_grad = True\n",
    "    return (xavier)\n",
    "\n",
    "\n",
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
    "bzh = (torch.zeros(h_dim))\n",
    "bzh.requires_grad = True\n",
    "\n",
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = (torch.zeros(X_dim))\n",
    "bhx.requires_grad = True\n",
    "\n",
    "def G(z):\n",
    "    h = torch.relu(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
    "    X = torch.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "Wxh = xavier_init(size=[X_dim, h_dim])\n",
    "bxh = (torch.zeros(h_dim))\n",
    "bxh.requires_grad = True\n",
    "\n",
    "Why = xavier_init(size=[h_dim, y_dim])\n",
    "bhy = (torch.zeros(y_dim))\n",
    "bhy.requires_grad = True\n",
    "\n",
    "def D(X):\n",
    "    h = torch.relu(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
    "    y = torch.sigmoid(h @ Why + bhy.repeat(h.size(0), 1))\n",
    "    return y\n",
    "\n",
    "\n",
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params\n",
    "\n",
    "\n",
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            data = p.grad.data\n",
    "            p.grad = (data.new().resize_as_(data).zero_())\n",
    "\n",
    "\n",
    "G_solver = optim.Adam(G_params, lr=1e-3)\n",
    "D_solver = optim.Adam(D_params, lr=1e-3)\n",
    "\n",
    "ones_label = (torch.ones(mb_size, y_dim))\n",
    "zeros_label = (torch.zeros(mb_size, y_dim))"
   ],
   "execution_count":38,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "for it in range(100000):\n",
    "    # Sample data\n",
    "    z = (torch.randn(mb_size, Z_dim))\n",
    "    X, _ = next(iter(train_dataloader))\n",
    "    X = X.reshape(X.shape[0],X_dim)\n",
    "    X = torch.as_tensor(X,dtype=torch.float)\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    #D_loss_real = nn.binary_cross_entropy(D_real, ones_label)\n",
    "    #D_loss_fake = nn.binary_cross_entropy(D_fake, zeros_label)\n",
    "    #D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    D_loss = -(torch.mean(torch.log(D_real) + torch.log(1. - D_fake)))\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    z = (torch.randn(mb_size, Z_dim))\n",
    "    G_sample = G(z)\n",
    "    #print(G_sample.shape)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    #G_loss = nn.binary_cross_entropy(D_fake, ones_label)\n",
    "\n",
    "    G_loss = -(torch.mean(torch.log(D_fake)))\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "        samples = G(z).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out\/'):\n",
    "            os.makedirs('out\/')\n",
    "\n",
    "        plt.savefig('out\/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)"
   ],
   "execution_count":39,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Iter-0; D_loss: 1.4279001951217651; G_loss: 2.307133197784424\n",
      "Iter-1000; D_loss: 0.014277681708335876; G_loss: 9.329833984375\n",
      "Iter-2000; D_loss: 0.005390492267906666; G_loss: 8.5023832321167\n",
      "Iter-3000; D_loss: 0.06562675535678864; G_loss: 5.705738544464111\n",
      "Iter-4000; D_loss: 0.06101506948471069; G_loss: 7.2230119705200195\n",
      "Iter-5000; D_loss: 0.4056508541107178; G_loss: 4.784977912902832\n",
      "Iter-6000; D_loss: 0.26737695932388306; G_loss: 4.271829605102539\n",
      "Iter-7000; D_loss: 0.35483354330062866; G_loss: 3.3798208236694336\n",
      "Iter-8000; D_loss: 0.45045316219329834; G_loss: 3.136461019515991\n",
      "Iter-9000; D_loss: 0.6955567598342896; G_loss: 2.6970341205596924\n",
      "Iter-10000; D_loss: 0.5434374213218689; G_loss: 2.5059731006622314\n",
      "Iter-11000; D_loss: 0.7650926113128662; G_loss: 3.5989723205566406\n",
      "Iter-12000; D_loss: 0.881645917892456; G_loss: 2.8203768730163574\n",
      "Iter-13000; D_loss: 0.7671983242034912; G_loss: 2.800879955291748\n",
      "Iter-14000; D_loss: 0.9357052445411682; G_loss: 2.273482322692871\n",
      "Iter-15000; D_loss: 0.7458423972129822; G_loss: 2.3813796043395996\n",
      "Iter-16000; D_loss: 0.9358415007591248; G_loss: 1.7655044794082642\n",
      "Iter-17000; D_loss: 0.8854233026504517; G_loss: 1.6052515506744385\n",
      "Iter-18000; D_loss: 0.7213204503059387; G_loss: 1.9662002325057983\n",
      "Iter-19000; D_loss: 0.9396842122077942; G_loss: 1.468003511428833\n",
      "Iter-20000; D_loss: 0.8378937244415283; G_loss: 1.57047438621521\n",
      "Iter-21000; D_loss: 0.9449833035469055; G_loss: 1.8474245071411133\n",
      "Iter-22000; D_loss: 0.8380125164985657; G_loss: 2.1448307037353516\n",
      "Iter-23000; D_loss: 0.7601007223129272; G_loss: 1.9021046161651611\n",
      "Iter-24000; D_loss: 0.8557473421096802; G_loss: 1.9041247367858887\n",
      "Iter-25000; D_loss: 0.755420446395874; G_loss: 1.8793489933013916\n",
      "Iter-26000; D_loss: 0.7370831966400146; G_loss: 2.1040353775024414\n",
      "Iter-27000; D_loss: 0.8781590461730957; G_loss: 1.6630287170410156\n",
      "Iter-28000; D_loss: 0.8724142909049988; G_loss: 1.8541793823242188\n",
      "Iter-29000; D_loss: 0.8329299688339233; G_loss: 1.6368985176086426\n",
      "Iter-30000; D_loss: 0.9794439673423767; G_loss: 1.699547290802002\n",
      "Iter-31000; D_loss: 0.8882610201835632; G_loss: 1.790722370147705\n",
      "Iter-32000; D_loss: 0.7941213846206665; G_loss: 1.904044270515442\n",
      "Iter-33000; D_loss: 0.803851306438446; G_loss: 1.6643866300582886\n",
      "Iter-34000; D_loss: 0.9745786190032959; G_loss: 1.9673223495483398\n",
      "Iter-35000; D_loss: 0.6662529110908508; G_loss: 1.9725141525268555\n",
      "Iter-36000; D_loss: 0.8441649675369263; G_loss: 2.0396058559417725\n",
      "Iter-37000; D_loss: 0.7748651504516602; G_loss: 1.841672420501709\n",
      "Iter-38000; D_loss: 0.7210541367530823; G_loss: 2.0302481651306152\n",
      "Iter-39000; D_loss: 0.6959762573242188; G_loss: 2.1681995391845703\n",
      "Iter-40000; D_loss: 0.7602871060371399; G_loss: 2.4835729598999023\n",
      "Iter-41000; D_loss: 0.8751777410507202; G_loss: 1.8726515769958496\n",
      "Iter-42000; D_loss: 0.7967803478240967; G_loss: 2.1510300636291504\n",
      "Iter-43000; D_loss: 0.8317291736602783; G_loss: 1.9865150451660156\n",
      "Iter-44000; D_loss: 0.7440255880355835; G_loss: 2.161395311355591\n",
      "Iter-45000; D_loss: 0.7682536244392395; G_loss: 2.0854015350341797\n",
      "Iter-46000; D_loss: 0.7858051061630249; G_loss: 2.078813076019287\n",
      "Iter-47000; D_loss: 0.7465168833732605; G_loss: 2.37345552444458\n",
      "Iter-48000; D_loss: 0.7171780467033386; G_loss: 2.107973098754883\n",
      "Iter-49000; D_loss: 0.7378212213516235; G_loss: 2.5207459926605225\n",
      "Iter-50000; D_loss: 0.7500543594360352; G_loss: 2.4794631004333496\n",
      "Iter-51000; D_loss: 0.7891177535057068; G_loss: 2.0680007934570312\n",
      "Iter-52000; D_loss: 0.7804456949234009; G_loss: 1.846051812171936\n",
      "Iter-53000; D_loss: 0.7638963460922241; G_loss: 2.0497183799743652\n",
      "Iter-54000; D_loss: 0.8776034712791443; G_loss: 2.31821870803833\n",
      "Iter-55000; D_loss: 0.7503257989883423; G_loss: 2.2829172611236572\n",
      "Iter-56000; D_loss: 0.8863484859466553; G_loss: 1.9088072776794434\n",
      "Iter-57000; D_loss: 0.6397542953491211; G_loss: 1.9771419763565063\n",
      "Iter-58000; D_loss: 0.9337758421897888; G_loss: 2.2758290767669678\n",
      "Iter-59000; D_loss: 0.46915483474731445; G_loss: 2.051034688949585\n",
      "Iter-60000; D_loss: 0.7881878018379211; G_loss: 2.0138931274414062\n",
      "Iter-61000; D_loss: 0.701901912689209; G_loss: 1.9817898273468018\n",
      "Iter-62000; D_loss: 0.698194146156311; G_loss: 2.0464212894439697\n",
      "Iter-63000; D_loss: 0.6049440503120422; G_loss: 1.9136189222335815\n",
      "Iter-64000; D_loss: 0.7830740809440613; G_loss: 2.230426788330078\n",
      "Iter-65000; D_loss: 0.5930055379867554; G_loss: 1.9016385078430176\n",
      "Iter-66000; D_loss: 0.6722142696380615; G_loss: 2.3445863723754883\n",
      "Iter-67000; D_loss: 0.9213058948516846; G_loss: 2.0961709022521973\n",
      "Iter-68000; D_loss: 0.6831291317939758; G_loss: 2.6674721240997314\n",
      "Iter-69000; D_loss: 0.7346019148826599; G_loss: 1.9897428750991821\n",
      "Iter-70000; D_loss: 0.6536145210266113; G_loss: 2.095477342605591\n",
      "Iter-71000; D_loss: 0.6748648285865784; G_loss: 2.2917416095733643\n",
      "Iter-72000; D_loss: 0.6575877666473389; G_loss: 2.0549604892730713\n",
      "Iter-73000; D_loss: 0.6794900298118591; G_loss: 2.3392276763916016\n",
      "Iter-74000; D_loss: 0.7988352179527283; G_loss: 2.17832350730896\n",
      "Iter-75000; D_loss: 0.6605069041252136; G_loss: 2.632096767425537\n",
      "Iter-76000; D_loss: 0.771103024482727; G_loss: 2.1452956199645996\n",
      "Iter-77000; D_loss: 0.7337515354156494; G_loss: 2.267730712890625\n",
      "Iter-78000; D_loss: 0.6375254392623901; G_loss: 1.9867146015167236\n",
      "Iter-79000; D_loss: 0.5913690328598022; G_loss: 2.4327199459075928\n",
      "Iter-80000; D_loss: 0.6077054738998413; G_loss: 2.0677080154418945\n",
      "Iter-81000; D_loss: 0.597375214099884; G_loss: 1.923370599746704\n",
      "Iter-82000; D_loss: 0.7548099756240845; G_loss: 2.1674368381500244\n",
      "Iter-83000; D_loss: 0.6177318096160889; G_loss: 1.9546843767166138\n",
      "Iter-84000; D_loss: 0.5601792931556702; G_loss: 2.11727237701416\n",
      "Iter-85000; D_loss: 0.6226897239685059; G_loss: 2.165846109390259\n",
      "Iter-86000; D_loss: 0.6555214524269104; G_loss: 1.9969958066940308\n",
      "Iter-87000; D_loss: 0.7305497527122498; G_loss: 2.603559732437134\n",
      "Iter-88000; D_loss: 0.6397069096565247; G_loss: 2.0437612533569336\n",
      "Iter-89000; D_loss: 0.6849167346954346; G_loss: 2.3947396278381348\n",
      "Iter-90000; D_loss: 0.6633244156837463; G_loss: 2.3649682998657227\n",
      "Iter-91000; D_loss: 0.6013026237487793; G_loss: 2.1528663635253906\n",
      "Iter-92000; D_loss: 0.5101701617240906; G_loss: 2.3506722450256348\n",
      "Iter-93000; D_loss: 0.6997575759887695; G_loss: 2.2333178520202637\n",
      "Iter-94000; D_loss: 0.631255030632019; G_loss: 2.061845541000366\n",
      "Iter-95000; D_loss: 0.6777549386024475; G_loss: 2.2098536491394043\n",
      "Iter-96000; D_loss: 0.5597249269485474; G_loss: 2.2431416511535645\n",
      "Iter-97000; D_loss: 0.6761465072631836; G_loss: 2.2849998474121094\n",
      "Iter-98000; D_loss: 0.7015753984451294; G_loss: 2.3462588787078857\n",
      "Iter-99000; D_loss: 0.7987508773803711; G_loss: 1.9114043712615967\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}